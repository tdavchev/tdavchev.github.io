

<!doctype html>
<html lang="en" class="no-js">
  <head>
    

<meta charset="utf-8">



<!-- begin SEO -->









<title>Publications - Todor Davchev</title>







<meta property="og:locale" content="en-US">
<meta property="og:site_name" content="Todor Davchev">
<meta property="og:title" content="Publications">


  <link rel="canonical" href="https://tdavchev.github.io/publications/">
  <meta property="og:url" content="https://tdavchev.github.io/publications/">





  <meta name="twitter:site" content="@yadrimz">
  <meta name="twitter:title" content="Publications">
  <meta name="twitter:description" content="PhD student, University of Edinburgh">
  <meta name="twitter:url" content="https://tdavchev.github.io/publications/">

  
    <meta name="twitter:card" content="summary">
    
  

  



  

  












  <script type="application/ld+json">
    {
      "@context" : "http://schema.org",
      "@type" : "Person",
      "name" : "Todor Davchev",
      "url" : "https://tdavchev.github.io",
      "sameAs" : null
    }
  </script>






<!-- end SEO -->


<link href="https://tdavchev.github.io/feed.xml" type="application/atom+xml" rel="alternate" title="Todor Davchev Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="https://tdavchev.github.io/assets/css/main.css">

<meta http-equiv="cleartype" content="on">
    

<!-- start custom head snippets -->

<link rel="apple-touch-icon" sizes="57x57" href="https://tdavchev.github.io/images/apple-touch-icon-57x57.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="60x60" href="https://tdavchev.github.io/images/apple-touch-icon-60x60.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="72x72" href="https://tdavchev.github.io/images/apple-touch-icon-72x72.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="76x76" href="https://tdavchev.github.io/images/apple-touch-icon-76x76.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="114x114" href="https://tdavchev.github.io/images/apple-touch-icon-114x114.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="120x120" href="https://tdavchev.github.io/images/apple-touch-icon-120x120.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="144x144" href="https://tdavchev.github.io/images/apple-touch-icon-144x144.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="152x152" href="https://tdavchev.github.io/images/apple-touch-icon-152x152.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="180x180" href="https://tdavchev.github.io/images/apple-touch-icon-180x180.png?v=M44lzPylqQ">
<link rel="icon" type="image/png" href="https://tdavchev.github.io/images/favicon-32x32.png?v=M44lzPylqQ" sizes="32x32">
<link rel="icon" type="image/png" href="https://tdavchev.github.io/images/android-chrome-192x192.png?v=M44lzPylqQ" sizes="192x192">
<link rel="icon" type="image/png" href="https://tdavchev.github.io/images/favicon-96x96.png?v=M44lzPylqQ" sizes="96x96">
<link rel="icon" type="image/png" href="https://tdavchev.github.io/images/favicon-16x16.png?v=M44lzPylqQ" sizes="16x16">
<link rel="manifest" href="https://tdavchev.github.io/images/manifest.json?v=M44lzPylqQ">
<link rel="mask-icon" href="https://tdavchev.github.io/images/safari-pinned-tab.svg?v=M44lzPylqQ" color="#000000">
<link rel="shortcut icon" href="/images/favicon.ico?v=M44lzPylqQ">
<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="https://tdavchev.github.io/images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="https://tdavchev.github.io/images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="https://tdavchev.github.io/assets/css/academicons.css"/>

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- end custom head snippets -->

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg"><a href="https://tdavchev.github.io/">Todor Davchev</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://tdavchev.github.io/publications/">Publications</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://tdavchev.github.io/teaching/">Teaching</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://tdavchev.github.io/year-archive/">Blog Posts</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://tdavchev.github.io/cv/">CV</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    



<div id="main" role="main">
  


  <div class="sidebar sticky">
  



<div itemscope itemtype="http://schema.org/Person">

  <div class="author__avatar">
    
    	<img src="https://tdavchev.github.io/images/profile_1.jpg" class="author__avatar" alt="Todor Davchev">
    
  </div>

  <div class="author__content">
    <h3 class="author__name">Todor Davchev</h3>
    <p class="author__bio">PhD student in Robot Learning, University of Edinburgh</p>
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li><i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> Edinburgh</li>
      
      
      
      
        <li><a href="mailto:t.davchev@gmail.com"><i class="fas fa-fw fa-envelope" aria-hidden="true"></i> Email</a></li>
      
      
       
      
        <li><a href="https://twitter.com/yadrimz"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
      
      
      
      
        <li><a href="https://www.linkedin.com/in/todor-davchev-66b9b148"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
      
      
      
      
      
      
        <li><a href="https://github.com/tdavchev"><i class="fab fa-fw fa-github" aria-hidden="true"></i> Github</a></li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <li><a href="https://scholar.google.com/citations?user=h_q7XhoAAAAJ&hl=en"><i class="fas fa-fw fa-graduation-cap"></i> Google Scholar</a></li>
      
      
      
        <li><a href="https://orcid.org/0000-0002-0584-5163"><i class="ai ai-orcid-square ai-fw"></i> ORCID</a></li>
      
      
      
    </ul>
  </div>
</div>

  
  </div>


  <div class="archive">
    
      <h1 class="page__title">Publications</h1>
    
    
<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="https://tdavchev.github.io/publication/2020-residual-lfd" rel="permalink">Residual Learning from Demonstration
</a>
      
    </h2>
    
    

        
          <p>Published in <i>arXiv preprint arXiv:2008.07682</i>, 2020 </p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><p>Contacts and friction are inherent to nearly all robotic manipulation tasks. Through the motor skill of insertion, we study how robots can learn to cope when these attributes play a salient role. In this work we propose residual learning from demonstration (rLfD), a framework that combines dynamic movement primitives (DMP) that rely on behavioural cloning with a reinforcement learning (RL) based residual correction policy. The proposed solution is applied directly in task space and operates on the full pose of the robot. We show that rLfD outperforms alternatives and improves the generalisation abilities of DMPs. We evaluate this approach by training an agent to successfully perform both simulated and real world insertions of pegs, gears and plugs into respective sockets.</p>
</p>
    
    
    
      <p>Recommended citation: Davchev, T., Luck, K.S., Burke, M., Meier, F., Schaal, S. and Ramamoorthy, S., 2020. Residual Learning from Demonstration. arXiv preprint arXiv:2008.07682. <a href="https://arxiv.org/pdf/2008.07682.pdf"><u>https://arxiv.org/pdf/2008.07682.pdf</u></a></p>
    

  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="https://tdavchev.github.io/publication/2019-vid-to-param" rel="permalink">Vid2Param: Modelling of Dynamics Parameters from Video
</a>
      
    </h2>
    
    

        
          <p>Published in <i>IEEE Robotics and Automation Letters</i>, 2019 </p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><p>Videos provide a rich source of information, but it is generally hard to extract dynamical parameters of interest. Inferring those parameters from a video stream would be beneficial for physical reasoning. Robots performing tasks in dynamic environments would benefit greatly from understanding the underlying environment motion, in order to make future predictions and to synthesize effective control policies that use this inductive bias. Online physical reasoning is therefore a fundamental requirement for robust autonomous agents. When the dynamics involves multiple modes (due to contacts or interactions between objects) and sensing must proceed directly from a rich sensory stream such as video, then traditional methods for system identification may not be well suited. We propose an approach wherein fast parameter estimation can be achieved directly from video. We integrate a physically based dynamics model with a recurrent variational autoencoder, by introducing an additional loss to enforce desired constraints. The model, which we call Vid2Param, can be trained entirely in simulation, in an end-to-end manner with domain randomization, to perform online system identification, and make probabilistic forward predictions of parameters of interest. This enables the resulting model to encode parameters such as position, velocity, restitution, air drag and other physical properties of the system. We illustrate the utility of this in physical experiments wherein a PR2 robot with a velocity constrained arm must intercept an unknown bouncing ball with partly occluded vision, by estimating the physical parameters of this ball directly from the video trace after the ball is released.</p>
</p>
    
    
    
      <p>Recommended citation: Asenov, M., Burke, M., Angelov, D., Davchev, T., Subr, K. and Ramamoorthy, S., 2019. Vid2Param: Modeling of Dynamics Parameters From Video. IEEE Robotics and Automation Letters, 5(2), pp.414-421. <a href="http://homepages.inf.ed.ac.uk/ksubr/Files/Papers/ICRA20Vid2Param.pdf"><u>http://homepages.inf.ed.ac.uk/ksubr/Files/Papers/ICRA20Vid2Param.pdf</u></a></p>
    

  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="https://tdavchev.github.io/publication/2019-structured-representations" rel="permalink">Learning Structured Representations of Spatial and Interactive Dynamics for Trajectory Prediction in Crowded Scenes.
</a>
      
    </h2>
    
    

        
          <p>Published in <i>arXiv preprint arXiv:1911.13044</i>, 2019 </p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><p>Context plays a significant role in the generation of motion for dynamic agents in interactive environments. This work proposes a modular method that utilises a learned model of the environment for motion prediction and explicitly allows for unsupervised adaptation of trajectory prediction models to unseen environments and new tasks by decoupling per-agent dynamics and environment modelling. Modelling both the spatial and dynamic aspects of a given environment alongside the per agent behaviour results in more informed motion prediction and allows for performance comparable to the state-of-the-art. We highlight the model prediction capability using a benchmark pedestrian prediction problem and a robot manipulation task and show that we can transfer the predictor across these tasks in a completely unsupervised way. The proposed approach allows for robust and label efficient forward modelling, and relaxes the need for full model re-training in new environments.</p>
</p>
    
    
    
      <p>Recommended citation: Davchev, Todor, Michael Burke, and Subramanian Ramamoorthy. Learning Structured Representations of Spatial and Interactive Dynamics for Trajectory Prediction in Crowded Scenes. arXiv preprint arXiv:1911.13044 (2019). <a href="https://arxiv.org/pdf/1911.13044.pdf"><u>https://arxiv.org/pdf/1911.13044.pdf</u></a></p>
    

  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="https://tdavchev.github.io/publication/2019-adversarial-transfer" rel="permalink">An Empirical Evaluation of Adversarial Robustness under Transfer Learning.
</a>
      
    </h2>
    
    

        
          <p>Published in <i>ICML 2019 Understanding and Improving Generalisation Workshop</i>, 2019 </p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><p>In this work, we evaluate adversarial robustness in the context of transfer learning from a source trained on CIFAR 100 to a target network trained on CIFAR 10. Specifically, we study the effects of using robust optimisation in the source and target networks. This allows us to identify transfer learning strategies under which adversarial defences are successfully retained, in addition to revealing potential vulnerabilities. We study the extent to which features learnt by a fast gradient sign method (FGSM) and its iterative alternative (PGD) can preserve their defence properties against black and white-box attacks under three different transfer learning strategies. We find that using PGD examples during training on the source task leads to more general robust features that are easier to transfer. Furthermore, under successful transfer, it achieves 5.2% more accuracy against white-box PGD attacks than suitable baselines. Overall, our empirical evaluations give insights on how well adversarial robustness under transfer learning can generalise.</p>
</p>
    
    
    
      <p>Recommended citation: Davchev, T., Korres, T., Fotiadis, S., Antonopoulos, N. and Ramamoorthy, S., 2019. An empirical evaluation of adversarial robustness under transfer learning. arXiv preprint arXiv:1905.02675 (2019). <a href="https://arxiv.org/pdf/1905.02675.pdf"><u>https://arxiv.org/pdf/1905.02675.pdf</u></a></p>
    

  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="https://tdavchev.github.io/publication/2016-modelling_entailment" rel="permalink">Modelling Entailment with Neural Networks
</a>
      
    </h2>
    
    

        
          <p>Published in <i>MSc Thesis</i>, 2016 </p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><p>Sentence classification is currently among the unresolved challenges of Natural Language Understanding and Machine Learning. In this thesis we focus on modelling entailment relations which can be considered as a sub-problem of sentence classification. We show that the results from the currently adopted Recurrent Neural Networks and Long Short-Term Memory models can be matched and even outperformed for recognising textual entailment. More specifically, we show that other techniques, such as Convolutional Neural Networks (CNNs), tackle the problem in a similar in terms of accuracy, however simpler in terms of feature engineering approach. We propose a novel Siamese-like 3-CNN-wide architecture. We extend that model by applying a variety of mathematical operations to the intermediate input of the third CNN. More precisely, we exploit the low dimensionality representation of the already processed initial inputs via a series of linear and multiplicative operands. We then show that our approach achieves better results than the existing techniques, however significantly increasing the size of the parameters trained. Nevertheless, our implementation has a modular and loosely coupled architecture.</p>
</p>
    
    
    
      <p>Recommended citation: Davchev, Todor. (2016). &quot;Modelling Entailment with Neural Networks.&quot; <i>MSc Thesis</i>. University of Edinburgh. <a href="http://tdavchev.github.io/files/MSc_Dissertation_Report.pdf"><u>http://tdavchev.github.io/files/MSc_Dissertation_Report.pdf</u></a></p>
    

  </article>
</div>


  </div>
</div>


    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->
<a href="/sitemap/">Sitemap</a>
<!-- end custom footer snippets -->

        

<div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    
    
      <li><a href="https://twitter.com/yadrimz"><i class="fab fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
    
    
    
      <li><a href="http://github.com/tdavchev"><i class="fab fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    <li><a href="https://tdavchev.github.io/feed.xml"><i class="fa fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2020 Todor Davchev. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/academicpages/academicpages.github.io">AcademicPages</a>, a fork of <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    <script src="https://tdavchev.github.io/assets/js/main.min.js"></script>




  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-36376375-4', 'auto');
  ga('send', 'pageview');
</script>






  </body>
</html>

