---
title: 'Tutorial on Stochastic Trajectory Prediction'
date: 2019-12-01
permalink: /posts/2019/12/blog-post-1/
tags:
  - trajectory generation
  - tensorflow
  - stochastic trajectory prediction
---

This tutorial is meant as an introduction to the problem of trajectory generation. It introduces several ways for modelling the motion of agents in pixel space and proposes several ways of preprocessing data. It follows the structure from its associated [GitHub repository](https://github.com/yadrimz/Stochastic-Futures-Prediction). Feel free to skip to the end to see the performance of 2 basic models.

Introduction
======

The acquisition of neural-based solutions such as Long Shrort-Term Memory [1] has become very common for modelling time series signals in the past few years. Some popular and well established examples include different applications of AI; and namely speech processing [2], language modelling [3], translation [4] among many others. In particular, the idea behind LSTMs is that they are good at mimicing the hidden dynamics behind a given short sequence. 

The overall structure of such networks (see the figure bellow, taken from [Chris Olah's](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) blog post which is a great introduction to LSTMs along with [Chapter 10 of the book "Deep Learning"](http://www.deeplearningbook.org/contents/rnn.html)) allows us to model each step while taking into account all previously considered ones in a given signal. In the context of text modelling, those signals can be all previously observed words in a given sentence where each step would consist of predicting one word by conditioning on all previously seen words. In this context, LSTMs are good at encapsulating the context behind a given sentence which makes them a good candidate for predicting its continuation.
![LSTM-Chris Olah](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)

Similarly, we can model the dynamics associated with the motion of pedestrians in crowded scenes, the behaviour of agents in different games, moving cars on a busy road etc. In such cases, the task would be to predict where each considered agent will be after some time. This post is an introduction to the use of LSTMs in such problems. 

We will focus on predicting the next few 2D positions (x,y) of pedestrians from annotated videos. We will assume a good understanding of what LSTMs are, the difficulties behind extracting meaningful feature representations as well as basic computer science knowledge, algebra and calculus, and some prior knowledge of using Tensorflow. We aim to conclude with a brief discussion of the pros and cons of using the introduced here use of LSTMS.

Anticipating the position of pedestrians in a given video sequence is often a challenging task even for humans. Motion is often dictated by some unspoken rules that are different across cultures and are in addition interpreted differently by people. For example, when walking in crowded scenes we might aim to avoid collisions with others, but we can also aim to reach someone specific and stop in front of them. In such cases it is relatively difficult for an observer to anticipate what would be the goal of someone walking in the scene in the next 5-10 minutes just by observing but we can relatively easily tell where a pedestrian aims to be in the next 5-10 seconds. Regardless, we often maintain a few hypothesis of where this person will be and thus modelling such motions using purely deterministic approaches (such as simply using standard LSTMs) can be very hard. An alternative solution would be to train a neural network to model the sufficient statistics of the distribution the next step is sampled from.

Initialization
------

We begin by installing tensorflow and downloading the github code described in this blog post. We then import everything necessary including all datasets we will use for training, followed by assigning all constant variables we will need for training the network.

`!pip install tensorflow==1.15.0`

`!git clone https://github.com/yadrimz/Stochastic-Futures-Prediction.git`

Problem Formulation
======
This work's focus is on predicting the future motion of an arbitrary number of observed agents (i.e. their behaviour) whose action spaces and objectives are unknown. More specifically, we focus on predicting the two-dimensional motion of agents in video sequences.

We assume we are given a history of two-dimensional position annotations and video frames as a sequence of RGB images. 
Each agent $a$ ($a \in [1 \ldots A]$, where $A$ is the maximum number of agents in the video) is represented by a state ($s_t^a$) which comprises xy-coordinates at time t, $s_t^a$ = $(x_t,y_t)_a$.
Given a sequence of $obs$ observed states $S = (s_{t-obs}, s_{t-obs+1}, \ldots, s_{t-1}, s_{t})$, we will formulate the prediction as an optimisation process, where the objective is to learn a posterior distribution $P(Y \vert S)$, of multiple agents future trajectories $Y$. Here an individual agent's future trajectory is defined as ($s_t^a = \{s_{t+1}^a, s_{t+2}^a, \ldots, s_{t+pred}^a\}$) for $pred$ steps ahead for every agent $a$ found in a given frame at time $t$.

{% highlight python %}
import os
import time

%tensorflow_version 1.x

import numpy as np
import tensorflow as tf

import utils.data_tools as data_tools
import utils.visualisation as visualisation
import utils.distributions as distributions

from models.lstm import BasicLSTM
from models.lstm import reset_graph
{% endhighlight %}

Datasets Used
======
Before considering the details around modelling such tasks, we should spend some time to consider the datasets we will use as well as the preprocessing routines we will consider.

In this post, we consider four different datasets and namely, ETH University, ETH Hotel [5] (see next 2 photos as examples), and Zara1,2 [7]. Photos of the first two can be seen below.

Data Processing
------
In these examples we are interested in figuring out the exact pixel location of each individual pedestrian (agent), as well as the associated frames we consider. All four datasets give us annotated positions but differ slightly in representation. Thus, as a first step we ensure they are aligned. 

This is common across datasets when they have been built by different groups and projects and have slight misalignments. All four datasets have been recorded on 25Hz and consist on average of 3000 frames. The ETH datasets are comprised of 750 agents each while Zara has two scenes each with 786 agents. All videos include people walking on their own, as well as pedestrians moving in groups in a nonlinear manner. However, some of the videos annotate trajectories in mm in a world reference frame and others have recorded them in pixel coordinates with (0,0) considered in the centre of each video frame. We will process all of them ensuring all positions are represented in pixel positions with (0,0) placed in the bottom left corner. We will further normalise them between 0 and 1 such that the size of the image or the roadwalk considered do not bias our solution.

Processing examples
------
To simplify this post we will consider transforming 1 of the 4 datasets and leave the rest out. The aim is to clarify how such processing is achieved. The rest of the processing is similar to the one described here and can be further found in the [GitHub repository of this post](https://github.com/yadrimz/Stochastic-Futures-Prediction). This part, however, is not  necessary to understand the details around the actual model.

<b>ETH Hotel</b>
is comprised of positions in world reference frame where we are interested in converting these to local, pixel reference frame. To do this, we are given the required homography.

{% highlight python %}
def world_to_image_frame(loc, Hinv):
  """
  Given H^-1 and (x, y, z) in world coordinates, returns (u, v, 1) in image
  frame coordinates.
  """
  loc = np.dot(Hinv, loc) # to camera frame
  return loc/loc[2] # to pixels (from millimeters)
{% endhighlight %}
Those interested in the mathematics behind the introduced conversion can read more about it in [Taku Komura's lecture slides](http://www.inf.ed.ac.uk/teaching/courses/cg/lectures/cg3_2013.pdf). Further we normalise the data using the minimum and maximum recorded values which results in the following method. 

{% highlight python %}
def mil_to_pixels(directory=["./data/ewap_dataset/seq_hotel"]):
    '''
    Preprocess the frames from the datasets.
    Convert values to pixel locations from millimeters
    obtain and store all frames data the actually used frames (as some are skipped), 
    the ids of all pedestrians that are present at each of those frames and the sufficient statistics.
    '''
    def collect_stats(agents):
        x_pos = []
        y_pos = []
        for agent_id in range(1, len(agents)):
            trajectory = [[] for _ in range(3)]
            traj = agents[agent_id]
            for step in traj:
                x_pos.append(step[1])
                y_pos.append(step[2])
        x_pos = np.asarray(x_pos)
        y_pos = np.asarray(y_pos)
        # takes the average over all points through all agents
        return [[np.min(x_pos), np.max(x_pos)], [np.min(y_pos), np.max(y_pos)]]

    Hfile = os.path.join(directory, "H.txt")
    obsfile = os.path.join(directory, "obsmat.txt")
    # Parse homography matrix.
    H = np.loadtxt(Hfile)
    Hinv = np.linalg.inv(H)
    # Parse pedestrian annotations.
    frames, pedsInFrame, agents = parse_annotations(Hinv, obsfile)
    # collect mean and std
    statistics = collect_stats(agents)
    norm_agents = []
    # collect the id, normalised x and normalised y of each agent's position
    pedsWithPos = []
    for agent in agents:
        norm_traj = []
        for step in agent:
            _x = (step[1] - statistics[0][0]) / (statistics[0][1] - statistics[0][0])
            _y = (step[2] - statistics[1][0]) / (statistics[1][1] - statistics[1][0])
            norm_traj.append([int(frames[int(step[0])]), _x, _y])

        norm_agents.append(np.array(norm_traj))

    return np.array(norm_agents), statistics, pedsInFrame
{% endhighlight %}

Lines 8 to 20 find the minimum and maximum values for the x and y positions of the agents. The called "obsmat.txt" file contains the annotated data and is comprised of the frame number, the pedestrian id, position in the x axis in the world frame, position in the y, z as well as their associated 3 velocities. More information can be found in the README.txt file within the dataset directory. In this post we are only interested in considering the frame, the pedestrian's id and their x and y positions. 

Lines 34-41 are associated with the ordering of pedestrians across frames along the pedestrian id.

Line 29 calls parse_annotations() parses the collected data and converts it to reference frame.


{% highlight python %}
def parse_annotations(Hinv, obsmat_txt):
    '''
    Parse the dataset and convert to image frames data.
    '''
    mat = np.loadtxt(obsmat_txt)
    num_peds = int(np.max(mat[:,1])) + 1
    peds = [np.array([]).reshape(0,4) for _ in range(num_peds)] # maps ped ID -> (t,x,y,z) path
    
    num_frames = (mat[-1,0] + 1).astype("int")
    num_unique_frames = np.unique(mat[:,0]).size
    recorded_frames = [-1] * num_unique_frames # maps timestep -> (first) frame
    peds_in_frame = [[] for _ in range(num_unique_frames)] # maps timestep -> ped IDs

    frame = 0
    time = -1
    blqk = False
    for row in mat:
        if row[0] != frame:
            frame = int(row[0])
            time += 1
            recorded_frames[time] = frame

        ped = int(row[1])

        peds_in_frame[time].append(ped)
        loc = np.array([row[2], row[4], 1])
        loc = to_image_frame(loc)
        loc = [time, loc[0], loc[1], loc[2]]
        peds[ped] = np.vstack((peds[ped], loc))

    return recorded_frames, peds_in_frame, peds

{% endhighlight %}

We can combine the preprocessing of all datasets in a single function. Ideally, we will save the preprocessed data and load it directly each time we need to use it. Once this is done we can then call a function that loads the preprocessed data in the format we need it in. It will be useful to split the trajectories in a chosen in advance sequence length. We can then easily compute the number of batches we will get if we specified a batch size too.

{% highlight python %}
BATCH_SIZE = 50
SEQUENCE_LENGTH = 8
agents_data, dicto, dataset_indices = \
  data_tools.preprocess(training_directories)

loaded_data, num_batches = \
  data_tools.load_preprocessed(agents_data, BATCH_SIZE, SEQUENCE_LENGTH)
{% endhighlight %}

Batching
------

After obtaining and pre-processing all the information we need to implement a routine for sampling random batches that ensure the samples will be comprised of unbroken sequences. One thing to keep in mind is that some of the sampled trajectories might be shorter than some given sequence length and others might be longer. In the former case, we would like to avoid such trajectories, while in the latter we want to split the trajectories in multiple samples. We will do this by defining a function called "next_batch()" that will take in as input the associated data, required batch size, a frame pointer that indicates the currently considered starting point, desired sequence length, maximum number of pedestrians in the considered sequence across all datasets, the current dataset pointer and whether or not we are sampling during inference or training time.

{% highlight python %}
def next_batch(_data, pointer, batch_size, sequence_length, infer=False):
    '''
    Function to get the next batch of points
    '''
    # List of source and target data for the current batch
    x_batch = []
    y_batch = []
    # For each sequence in the batch
    for i in range(batch_size):
        # Extract the trajectory of the pedestrian pointed out by pointer
        traj = _data[pointer]
        # Number of sequences corresponding to his trajectory
        n_batch = int(traj.shape[0] / (sequence_length+1))
        # Randomly sample an index from which his trajectory is to be considered
        if not infer:
            idx = random.randint(0, traj.shape[0] - sequence_length - 1)
        else:
            idx = 0

        # Append the trajectory from idx until sequence_length into source and target data
        x_batch.append(np.copy(traj[idx:idx+sequence_length, :]))
        y_batch.append(np.copy(traj[idx+1:idx+sequence_length+1, :]))

        if random.random() < (1.0/float(n_batch)):
            # Adjust sampling probability
            # if this is a long datapoint, sample this data more with
            # higher probability
            pointer = tick_batch_pointer(pointer, len(_data))

    return x_batch, y_batch, pointer
{% endhighlight %}

Now that we have ensured we have the required data processed and have an associated batching function we can focus on building the actual model.

Methodology - Stochastic LSTMs
======

In this blog's experiments we will utilise the aforementioned (x,y) coordinate representations as input to the network. Since each of these coordinate representations is associated with a specific agent who will interact with each other, it is important to separate the associated sequences and acknowledge that each prediction will be dependent on the previous sequences observed for a given agent.


Implementation Details
------

As we already mentioned, we assume a good understanding of LSTMs. As input to the network we use a sequence of positions of a given agent and each step will be converted to a 128 dimensional feature vector. This conversion happens through a linear operation and a nonlinear, [ReLU (Rectified Linear Output)](http://cs231n.github.io/neural-networks-1/) activation.

{% highlight python %}
def embed_inputs(self, inputs, embedding_w, embedding_b):
  # embed the inputs
  with tf.name_scope("Embed_inputs"):
    embedded_inputs = []
    for x in inputs:
        # Each x is a 2D tensor of size numPoints x 2
        embedded_x = tf.nn.relu(tf.add(tf.matmul(x, embedding_w), embedding_b))
        embedded_inputs.append(embedded_x)

    return embedded_inputs
{% endhighlight %}
Now that we have a relatively good representation of the input we can feed it through the LSTM model. To do this, we will use 128 dimensional hidden cell state. We chose the hyperparameterisation proposed in [6], [12] where the authors chose them using cross-validation applied on a syntetic dataset. We use learning rate of 0.003, with annealing term of 0.95 and use RMS-prop [5] and L2 regularisation with $\lambda=0.5$ and clip our gradients between -10 and 10.

{% highlight python %}
NUM_EPOCHS = 100
DECAY_RATE = 0.95
GRAD_CLIP = 10
LR = 0.003
NUM_UNITS = 128
EMBEDDING = 128
MODE = 'train'

SAVE_PATH='save'

avg_time = 0 # used for printing
avg_loss = 0 # used for printing
{% endhighlight %}

We achieve this task by updating the associated with the LSTM cell state at each step as follows:

{% highlight python %}
def lstm_advance(self, embedded_inputs, cell, scope_name="LSTM"):
  # advance the lstm cell state with one for each entry
  with tf.variable_scope(scope_name) as scope:
    state = self.initial_state
    outputs = []
    for i, inp in enumerate(embedded_inputs):
      if i > 0:
        scope.reuse_variables()
      output, last_state = cell(inp, state)
      outputs.append(output)
 
    return outputs, last_state
{% endhighlight %}

Having done this, we can now convert the output from the LSTM in a 5 dimensional output.

{% highlight python %}
def final_layer(self, outputs, output_w, output_b):
  with tf.name_scope("Final_layer"):
    # Apply the linear layer. Output would be a 
    # tensor of shape 1 x output_size
    output = tf.reshape(tf.concat(outputs, 1), [-1, self.num_units])
    output = tf.nn.xw_plus_b(output, output_w, output_b)
    return output
{% endhighlight %}

We will use this output to define the distribution we will sample a predicted position $y_t$ and namely a 2D Gaussian distribution with a mean value $\mu = [\mu_x, \mu_y]$, standard deviation $\sigma = [\sigma_x, \sigma_y]$ and correlation $\rho$, similar to the described approach in [8].
